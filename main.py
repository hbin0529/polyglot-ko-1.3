# main.py

from model import load_model

# âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def build_prompt(user_input):
    return f"""ë‹¹ì‹ ì€ ì¹œì ˆí•œ ETL ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.
ì‚¬ìš©ìì™€ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ì¤‘ì…ë‹ˆë‹¤.

ğŸ‘¤ ì‚¬ìš©ì: {user_input}
ğŸ¤– AI:"""

# âœ… ì‹¤í–‰ ë¡œì§
def main():
    tokenizer, model = load_model()

    print("ğŸ¤– ETL ì „ë¬¸ê°€ AIì™€ ëŒ€í™”í•´ë³´ì„¸ìš”! ì¢…ë£Œí•˜ë ¤ë©´ 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.")

    while True:
        user_input = input("ğŸ‘¤ ì‚¬ìš©ì: ")
        if user_input.strip().lower() in ["exit", "quit"]:
            print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        prompt = build_prompt(user_input)
        encoded = tokenizer(prompt, return_tensors="pt")
        input_ids = encoded["input_ids"]
        attention_mask = encoded["attention_mask"]

        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=10000,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            temperature=0.8,
            top_p=0.95
        )

        # ğŸ¤– AI ì‘ë‹µ ì¶”ì¶œ
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # response_lines = response.split("ğŸ¤– AI:")
        answer = response.strip()

        print(f"ğŸ§  AI ì‘ë‹µ: {answer}\n")
        # break

if __name__ == "__main__":
    main()